from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Create pipeline for chat
chat_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)

# System and User Prompts
system_prompt = "### System:\nYou are a helpful assistant specialized in science.\n"
user_prompt = "Explain the concept of photosynthesis in simple terms.\n"

input_prompt = system_prompt + "### User:\n" + user_prompt + "### Assistant:\n"
response = chat_pipeline(input_prompt, max_length=200, do_sample=True, temperature=0.7)
print(response[0]['generated_text'])
